{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-21T19:27:43.674343Z",
     "start_time": "2025-09-21T19:27:43.652386Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "import joblib\n",
    "import mne\n",
    "import antropy as ant\n",
    "import scipy.stats as sp_stats\n",
    "import mne_features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6273004d5760491e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# General vars\n",
    "sr = 1000  # Hz\n",
    "window_size = 250 # samples\n",
    "mtl_path = 'P%s_mtl_clean.fif'\n",
    "all_subjects = ['01', '02', '03'] # example ids\n",
    "depth_model = joblib.load(r'depth_model.pkl')\n",
    "depth_channels = ['RAH1', 'LAH1', 'RA1', 'LA1', 'LEC1', 'REC1', 'RPHG1', 'LPHG1', 'RMH1', 'LMH1', 'RAH2', 'LAH2', 'RA2', 'LA2', 'LEC2', 'REC2', 'RPHG2', 'LPHG2', 'RMH2', 'LMH2']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34415b17fbe1bb7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_depth_features(epochs, subj):\n",
    "    \"\"\"\n",
    "    Extracts features for the depth model from epoched data.\n",
    "\n",
    "    Args:\n",
    "        epochs (np.ndarray or List): 2D array-like structure (n_epochs, n_samples)\n",
    "                                     containing the signal windows.\n",
    "        subj (str): The subject identifier.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one row per epoch and one column per feature.\n",
    "    \"\"\"\n",
    "    mobility, complexity = ant.hjorth_params(epochs, axis=1)\n",
    "    feat = {\n",
    "        'subj': np.full(len(epochs), subj),\n",
    "        'epoch_id': np.arange(len(epochs)),\n",
    "        'kurtosis': sp_stats.kurtosis(epochs, axis=1),\n",
    "        'hjorth_mobility': mobility,\n",
    "        'hjorth_complexity': complexity,\n",
    "        'ptp_amp': np.ptp(epochs, axis=1),\n",
    "        'samp_entropy': np.apply_along_axis(ant.sample_entropy, axis=1, arr=epochs)\n",
    "    }\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    feat = pd.DataFrame(feat)\n",
    "    \n",
    "    # Teager-Kaiser energy\n",
    "    kaiser = mne_features.univariate.compute_teager_kaiser_energy(np.array(epochs))\n",
    "    reshaped_list = np.array(kaiser).reshape(-1, 12)\n",
    "    X_kaiser = pd.DataFrame(reshaped_list)\n",
    "    # rename columns\n",
    "    X_kaiser.columns = [\n",
    "        f'teager_kaiser_energy_{i}_mean' if j % 2 == 0 else f'teager_kaiser_energy_{i}_std'\n",
    "        for i in range(6) for j in range(2)\n",
    "    ]\n",
    "\n",
    "    feat = pd.concat([feat, X_kaiser], axis=1)\n",
    "    return feat\n",
    "\n",
    "def raw_chan_to_feat(raw, chan, subj, depth):\n",
    "    \"\"\"\n",
    "    Processes a single channel from a raw file, epochs it, and extracts features.\n",
    "\n",
    "    This function normalizes the channel, segments it into 250ms windows,\n",
    "    and calls the appropriate feature extraction function (zEEG or depth).\n",
    "    Finally, it adds global channel-level features to all epochs.\n",
    "\n",
    "    Args:\n",
    "        raw (mne.io.Raw): The MNE raw object.\n",
    "        chan (str): The name of the channel to process.\n",
    "        subj (str): The subject identifier.\n",
    "        depth (bool): Flag. If True, extract depth features.\n",
    "                      If False, extract zEEG features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of features, one row per valid epoch.\n",
    "    \"\"\"\n",
    "    epochs = []\n",
    "    chan_raw = raw.copy().pick([chan]).get_data(reject_by_annotation='NaN').flatten()\n",
    "    \n",
    "    # normalize chan\n",
    "    chan_norm = (chan_raw - np.nanmean(chan_raw)) / np.nanstd(chan_raw)\n",
    "    \n",
    "    # run over all 250ms epochs (exclude last second)\n",
    "    for i in range(0, len(chan_norm) - sr, window_size):\n",
    "        if not np.isnan(chan_norm[i: i + window_size]).any():\n",
    "            epochs.append(chan_norm[i: i + window_size])\n",
    "\n",
    "    if depth:\n",
    "        curr_feat = extract_depth_features(epochs, subj)\n",
    "    else: # zeeg\n",
    "        curr_feat = extract_zeeg_features(epochs, subj, raw.info['sfreq'])\n",
    "    \n",
    "    # add channel features for all epochs\n",
    "    chan_feat = {\n",
    "        'chan_name': chan,\n",
    "        'chan_ptp': np.ptp(chan_norm[~np.isnan(chan_norm)]),\n",
    "        'chan_skew': sp_stats.skew(chan_norm[~np.isnan(chan_norm)]),\n",
    "        'chan_kurt': sp_stats.kurtosis(chan_norm[~np.isnan(chan_norm)]),\n",
    "    }\n",
    "\n",
    "    for feat in chan_feat.keys():\n",
    "        curr_feat[feat] = chan_feat[feat]\n",
    "\n",
    "    return curr_feat\n",
    "\n",
    "def get_depth_pred(subjects, threshold=0.8, min_channels=2):\n",
    "    \"\"\"\n",
    "    Generates depth model predictions (labels) for a list of subjects.\n",
    "\n",
    "    This function iterates through subjects, loads data, extracts features\n",
    "    from target channels, and generates a binary prediction for each epoch\n",
    "    based on a consensus of channels.\n",
    "\n",
    "    Args:\n",
    "        subjects: List of subject IDs.\n",
    "        threshold (float): Probability threshold for a single channel to be considered \"active\".\n",
    "        min_channels (int): The minimum number of active channels required to\n",
    "                            label an epoch as positive (1).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are subject IDs and values are np.ndarrays\n",
    "              of binary predictions (one per epoch).\n",
    "    \"\"\"\n",
    "    y_all = {}\n",
    "    for subj in subjects:\n",
    "        raw = mne.io.read_raw(mtl_path % subj)\n",
    "        # Find channels that exist in *both* the raw file and our target list\n",
    "        curr_chans = [chan for chan in raw.ch_names if chan in depth_channels]\n",
    "        y_curr = None\n",
    "        # Predict and sum over channels\n",
    "        for chan in curr_chans:\n",
    "            curr_feat = raw_chan_to_feat(raw, chan, subj, depth=True)\n",
    "            predictions = depth_model.predict_proba(curr_feat[depth_model.get_booster().feature_names])\n",
    "            if y_curr is None:\n",
    "                y_curr = (predictions[:, 1] >= threshold).astype(int)\n",
    "            else:\n",
    "                y_curr += (predictions[:, 1] >= threshold).astype(int)\n",
    "\n",
    "        # at least X channels should be above threshold\n",
    "        y_curr[y_curr <= min_channels - 1] = 0\n",
    "        y_curr[y_curr > min_channels - 1] = 1\n",
    "        y_all[subj] = y_curr\n",
    "\n",
    "    return y_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84e0111c4579ece0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_zeeg_features(epochs, subj, sr):\n",
    "    \"\"\"\n",
    "    Extracts a comprehensive set of zEEG features from epoched data.\n",
    "\n",
    "    Calculates features across several domains:\n",
    "    - Basic statistical (mean, std, ptp, etc.)\n",
    "    - Fractal / nonlinear (Higuchi, Katz, Hjorth)\n",
    "    - Entropy (Sample, Spectral, SVD)\n",
    "    - Power (Absolute, Normalized band power, band ratios)\n",
    "    - Energy (Band energy, band ratios)\n",
    "    - Wavelet energy\n",
    "    - Teager-Kaiser energy\n",
    "\n",
    "    Args:\n",
    "        epochs (np.ndarray or List): 2D array-like (n_epochs, n_samples).\n",
    "        subj (str): The subject identifier.\n",
    "        sr (int): The sampling rate in Hz.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one row per epoch and one column per feature.\n",
    "    \"\"\"\n",
    "    data = np.array(epochs)\n",
    "    uni = mne_features.univariate\n",
    "\n",
    "    # Frequency bands configuration\n",
    "    bands = {\n",
    "        'theta': (4, 8), 'alpha': (8, 12), 'sigma': (12, 16),\n",
    "        'beta': (16, 30), 'gamma': (30, 100), 'fast': (100, 300)\n",
    "    }\n",
    "    band_list = ['theta', 'alpha', 'sigma', 'beta', 'gamma', 'fast']\n",
    "    psd_params = {'psd_method': 'welch', 'psd_params': None}\n",
    "\n",
    "    # === Basic statistical & complexity features ===\n",
    "    basic_features = {\n",
    "        'ptp_amp': uni.compute_ptp_amp(data),\n",
    "        'mean': uni.compute_mean(data),\n",
    "        'std': uni.compute_std(data),\n",
    "        'variance': uni.compute_variance(data),\n",
    "        'skewness': uni.compute_skewness(data),\n",
    "        'kurtosis': uni.compute_kurtosis(data),\n",
    "        'quantile': uni.compute_quantile(data, q=0.75),\n",
    "        'rms': uni.compute_rms(data),\n",
    "        'line_length': uni.compute_line_length(data),\n",
    "        'zero_crossings': uni.compute_zero_crossings(data, threshold=np.finfo(float).eps),\n",
    "    }\n",
    "\n",
    "    # === Fractal & nonlinear features ===\n",
    "    complexity_features = {\n",
    "        'higuchi_fd': uni.compute_higuchi_fd(data, kmax=10),\n",
    "        'katz_fd': uni.compute_katz_fd(data),\n",
    "        'hurst_exp': uni.compute_hurst_exp(data),\n",
    "        'hjorth_mobility': uni.compute_hjorth_mobility(data),\n",
    "        'hjorth_complexity': uni.compute_hjorth_complexity(data),\n",
    "        'hjorth_mobility_spect': uni.compute_hjorth_mobility_spect(sr, data, normalize=False, **psd_params),\n",
    "        'hjorth_complexity_spect': uni.compute_hjorth_complexity_spect(sr, data, normalize=False, **psd_params),\n",
    "    }\n",
    "\n",
    "    # === Entropy features ===\n",
    "    entropy_features = {\n",
    "        'app_entropy': uni.compute_app_entropy(data, emb=2, metric='chebyshev'),\n",
    "        'samp_entropy': uni.compute_samp_entropy(data, emb=2, metric='chebyshev'),\n",
    "        'spect_entropy': uni.compute_spect_entropy(sr, data, **psd_params),\n",
    "        'svd_entropy': uni.compute_svd_entropy(data, tau=2, emb=10),\n",
    "        'svd_fisher_info': uni.compute_svd_fisher_info(data, tau=2, emb=10),\n",
    "        'decorr_time': uni.compute_decorr_time(sr, data),\n",
    "    }\n",
    "\n",
    "    # === Power features ===\n",
    "    abspow = uni.compute_pow_freq_bands(sr, data, {'total': (0.1, 500)}, False, psd_method='multitaper')\n",
    "\n",
    "    # Combine basic features\n",
    "    df_basic = pd.DataFrame({**basic_features, **complexity_features, **entropy_features, 'abspow_': abspow})\n",
    "\n",
    "    # === Spectral slope ===\n",
    "    slope = uni.compute_spect_slope(sr, data, fmin=0.1, fmax=50, with_intercept=True, psd_method='welch')\n",
    "    df_slope = pd.DataFrame(\n",
    "        np.array(slope).reshape(-1, 4),\n",
    "        columns=['spect_slope_intercept', 'spect_slope_slope', 'spect_slope_MSE', 'spect_slope_R2']\n",
    "    )\n",
    "\n",
    "    # === Frequency band power (normalized) ===\n",
    "    pow_bands = uni.compute_pow_freq_bands(\n",
    "        data=data, sfreq=sr, freq_bands=bands, normalize=True,\n",
    "        ratios=None, psd_method='multitaper', log=False\n",
    "    )\n",
    "    df_pow = pd.DataFrame(\n",
    "        np.array(pow_bands).reshape(-1, len(bands)),\n",
    "        columns=[f'pow_freq_bands_{b}' for b in band_list]\n",
    "    )\n",
    "    # Add all band ratios\n",
    "    for b1 in band_list:\n",
    "        for b2 in band_list:\n",
    "            if b1 != b2:\n",
    "                df_pow[f'pow_freq_bands_{b1}/{b2}'] = df_pow[f'pow_freq_bands_{b1}'] / df_pow[f'pow_freq_bands_{b2}']\n",
    "\n",
    "    # === Frequency band energy ===\n",
    "    energy = uni.compute_energy_freq_bands(sr, data, freq_bands=bands)\n",
    "    df_energy = pd.DataFrame(\n",
    "        np.array(energy).reshape(-1, len(bands)),\n",
    "        columns=[f'energy_freq_bands_{b}' for b in band_list]\n",
    "    )\n",
    "\n",
    "    # Add two-letter abbreviation ratios\n",
    "    for b1 in band_list:\n",
    "        for b2 in band_list:\n",
    "            if b1 != b2 and f'energy_freq_bands_{b2[0]}{b1[0]}' not in df_energy.columns:\n",
    "                df_energy[f'energy_freq_bands_{b1[0]}{b2[0]}'] = (\n",
    "                    df_energy[f'energy_freq_bands_{b1}'] / df_energy[f'energy_freq_bands_{b2}']\n",
    "                )\n",
    "\n",
    "    # === Wavelet features ===\n",
    "    wave = uni.compute_wavelet_coef_energy(data, wavelet_name='db4')\n",
    "    df_wave = pd.DataFrame(\n",
    "        np.array(wave).reshape(-1, 5),\n",
    "        columns=[f'wavelet_coef_energy_{i}' for i in range(5)]\n",
    "    )\n",
    "\n",
    "    # === Teager-Kaiser energy ===\n",
    "    kaiser = uni.compute_teager_kaiser_energy(data)\n",
    "    df_kaiser = pd.DataFrame(\n",
    "        np.array(kaiser).reshape(-1, 12),\n",
    "        columns=[f'teager_kaiser_energy_{i}_{stat}' for i in range(6) for stat in ['mean', 'std']]\n",
    "    )\n",
    "\n",
    "    # === Combine all features ===\n",
    "    df = pd.concat([df_basic, df_slope, df_energy, df_kaiser, df_pow, df_wave], axis=1)\n",
    "    df.insert(0, 'subj', subj)\n",
    "    df.insert(1, 'epoch_id', np.arange(len(df)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_all_features_per_chan(chan, subjects):\n",
    "    \"\"\"\n",
    "    Runs feature extraction for a *single channel* across all subjects.\n",
    "\n",
    "    Iterates through a list of subjects, loads their raw data, and calls\n",
    "    raw_chan_to_feat to extract zEEG features for the specified channel.\n",
    "\n",
    "    Args:\n",
    "        chan (str): The name of the channel to extract features for (e.g., \"zeeg1\").\n",
    "        subjects (List[str]): List of subject IDs to process.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are subject IDs and values are the\n",
    "              feature DataFrames returned by raw_chan_to_feat.\n",
    "    \"\"\"\n",
    "    all_features = {}\n",
    "    for subj in subjects:\n",
    "        raw = mne.io.read_raw(mtl_path % subj)\n",
    "        curr_feat = raw_chan_to_feat(raw, chan, subj, depth=False)\n",
    "        all_features[subj] = curr_feat\n",
    "\n",
    "    return all_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "415a71ee795f30d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run and create the pkl for the next model\n",
    "y_all = get_depth_pred(all_subjects)\n",
    "zeeg1_all = get_all_features_per_chan('zeeg1', all_subjects)\n",
    "zeeg2_all = get_all_features_per_chan('zeeg2', all_subjects)\n",
    "subj_data = {}\n",
    "for subj in all_subjects:\n",
    "    subj_data[subj] = {'zeeg1': zeeg1_all[subj], 'zeeg2': zeeg2_all[subj], 'y': y_all[subj]}\n",
    "\n",
    "# save for later analysis\n",
    "joblib.dump(subj_data, 'zeeg_training_data.pkl')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d591d2ef847867b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### zEEG model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b79ceb5f7a84518e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subj_data = joblib.load('zeeg_training_data.pkl')\n",
    "# combine all subjects\n",
    "symmetric = True\n",
    "x = pd.DataFrame()\n",
    "y = np.array([])\n",
    "for subj in all_subjects:\n",
    "    zeeg1_subj = subj_data[subj]['zeeg1']\n",
    "    zeeg2_subj = subj_data[subj]['zeeg2']\n",
    "    y_subj = subj_data[subj]['y']\n",
    "    zeeg1_subj.reset_index(drop=True, inplace=True)\n",
    "    zeeg2_subj.reset_index(drop=True, inplace=True)\n",
    "    x_subj = pd.concat([zeeg1_subj, zeeg2_subj], axis=1, ignore_index=True)\n",
    "    x_subj.columns = [f'zeeg1_{col}' for col in zeeg1_subj.columns] + [f'zeeg2_{col}' for col in zeeg2_subj.columns]\n",
    "    x = pd.concat([x, x_subj], ignore_index=True)\n",
    "    if symmetric:\n",
    "        x_sym = pd.concat([zeeg2_subj, zeeg1_subj], axis=1, ignore_index=True)\n",
    "        x_sym.columns = x_subj.columns\n",
    "        x = pd.concat([x, x_sym], ignore_index=True)\n",
    "        y = np.concatenate((y, y_subj))\n",
    "    y = np.concatenate((y, y_subj))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ed02ce56d69bf2a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# balance data in each subject\n",
    "x['pred'] = y\n",
    "sampled_data_0 = pd.DataFrame()\n",
    "sampled_data_1 = pd.DataFrame()\n",
    "max_samples = 3000\n",
    "for subj in all_subjects:\n",
    "    n_EDs = x[(x['zeeg1_subj'] == subj) & (x['pred'] == 1)].shape[0]\n",
    "    sample_count = min(max_samples, n_EDs)\n",
    "    # Get negative samples for this subject\n",
    "    sampled_data_0 = pd.concat([sampled_data_0, x[(x['zeeg1_subj'] == subj) & (x['pred'] == 0)].sample(sample_count, replace=True, random_state=8)])\n",
    "    # Get positive samples for this subject\n",
    "    sampled_data_1 = pd.concat([sampled_data_1, x[(x['zeeg1_subj'] == subj) & (x['pred'] == 1)].sample(sample_count, replace=True, random_state=8)])\n",
    "\n",
    "# --- Final Assembly of Balanced Data ---\n",
    "sampled_data = pd.concat([sampled_data_1, sampled_data_0], ignore_index=True)\n",
    "x = sampled_data.drop(columns='pred')\n",
    "y = sampled_data['pred']\n",
    "sampled_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "334df2d76cb2c4ef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# choose features\n",
    "meta_data = ['subj', 'epoch_id', 'chan_name', 'epoch']\n",
    "x_feat = x[x.columns[~x.columns.str.contains('|'.join(meta_data))]]\n",
    "\n",
    "# Initialize containers\n",
    "metrics = {'accuracy': [], 'precision': [], 'sensitivity': [], 'specificity': [], 'f1': [], 'ROCAUC': [], 'PRAUC': []}\n",
    "# Store all predictions for aggregate plots\n",
    "all_y_true = []\n",
    "all_y_prob = []\n",
    "\n",
    "# 5-fold stratified Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=8)\n",
    "for train_index, test_index in kf.split(x_feat, y):\n",
    "    model = xgb.XGBClassifier() \n",
    "    x_train_fold, x_test_fold = x_feat.iloc[train_index], x_feat.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "    \n",
    "    model.fit(x_train_fold, y_train_fold)\n",
    "    y_prob = model.predict_proba(x_test_fold)[:, 1]  # probabilities for class 1\n",
    "    y_pred = (y_prob > 0.5).astype(int)  # thresholding at 0.5\n",
    "    y_true = y_test_fold\n",
    "    all_y_true.extend(y_true)\n",
    "    all_y_prob.extend(y_prob)\n",
    "\n",
    "    # save metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics['accuracy'].append(accuracy_score(y_true, y_pred))\n",
    "    metrics['precision'].append(precision_score(y_true, y_pred))\n",
    "    metrics['sensitivity'].append(recall_score(y_true, y_pred))\n",
    "    metrics['f1'].append(f1_score(y_true, y_pred))\n",
    "    metrics['specificity'].append(tn / (tn + fp))\n",
    "    metrics['ROCAUC'].append(roc_auc_score(y_true, y_prob))\n",
    "    metrics['PRAUC'].append(average_precision_score(y_true, y_prob))\n",
    "\n",
    "# Create results table\n",
    "results = pd.DataFrame(metrics)\n",
    "results.loc['mean'] = results.mean()\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a274da9098faa50a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot ROC and PR curves\n",
    "fpr, tpr, _ = roc_curve(all_y_true, all_y_prob)\n",
    "precision, recall, _ = precision_recall_curve(all_y_true, all_y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd176fd1ffd137a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Application on non invasive data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11190aadb3a0dfe2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subjects_HC = ['HC1', 'HC2', 'HC3']  # example healthy control ids\n",
    "subjects_EPI = ['EPI1', 'EPI2', 'EPI3']  # example epilepsy patient ids\n",
    "# assume features are ready using the get_zeeg_features function\n",
    "hc = joblib.load(\"v1_HC.pkl\")\n",
    "epi = joblib.load(\"v1_EPI.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20fb052deef8ba5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_rate_per_bin(model, channel_1, channel_2, threshold, bin_length, sec_limit):\n",
    "    \"\"\"\n",
    "    Computes EDs-per-minute (SPM) for HC and EPI groups from feature data.\n",
    "\n",
    "    This function iterates through subjects, bins their data, runs the model,\n",
    "    applies a refractory period, and calculates the SPM for each bin.\n",
    "\n",
    "    Args:\n",
    "        model: The trained XGBoost model object.\n",
    "        channel_1 (str): Name of the first zeeg channel key (e.g., 'E227').\n",
    "        channel_2 (str): Name of the second zeeg channel key (e.g., 'E254').\n",
    "        threshold (float): Probability threshold for ED detection (0 to 1).\n",
    "        bin_length_min (int): Duration of each analysis bin in minutes.\n",
    "        refractory_sec (int): Refractory period in seconds after a detection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with [\"subject\", \"EDs_per_min\", \"group\"]\n",
    "                      for each bin of each subject.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling assumptions: 4 Hz -> 240 samples per minute\n",
    "    sampling_hz = 4\n",
    "    samples_per_min = sampling_hz * 60  # 240\n",
    "    bin_stride = bin_length * samples_per_min\n",
    "    refractory = int(sec_limit * sampling_hz)  # skip window after a detection\n",
    "\n",
    "    # Optional caches for downstream analysis, if needed later\n",
    "    proba_by_bin = {}  # (group, subj, part) -> np.ndarray of P(class=1)\n",
    "    tr_by_bin = {}     # (group, subj, part) -> np.ndarray of 0/1 after threshold + refractory\n",
    "\n",
    "    rows = []  # rows for the output DataFrame\n",
    "\n",
    "    def process_group(group_label, subjects, store):\n",
    "        \"\"\"\n",
    "        Run inference per subject in a group, split into bins, threshold with refractory,\n",
    "        and accumulate EDs-per-minute for each bin into rows.\n",
    "        \"\"\"\n",
    "        for subj in subjects:\n",
    "            # Build feature matrix for subject by concatenating zEEG channels\n",
    "            zeeg1 = store[subj][channel_1]\n",
    "            zeeg2 = store[subj][channel_2]\n",
    "            curr_feat = pd.concat([zeeg1, zeeg2], axis=1, ignore_index=True)\n",
    "            curr_feat.columns = [f\"zeeg1_{c}\" for c in zeeg1.columns] + [f\"zeeg2_{c}\" for c in zeeg2.columns]\n",
    "\n",
    "            # Enforce exact feature order known to the trained model\n",
    "            feat_names = model.get_booster().feature_names\n",
    "            X_all = curr_feat[feat_names]\n",
    "\n",
    "            part = 0\n",
    "            n = len(X_all)\n",
    "            for start in range(0, n, bin_stride):\n",
    "                stop = start + bin_stride\n",
    "                X = X_all.iloc[start:stop, :]\n",
    "\n",
    "                # Skip very short tail bins (< 50% of target size) to reduce noise\n",
    "                if len(X) < (bin_stride // 2):\n",
    "                    continue\n",
    "\n",
    "                key = (group_label, subj, part)\n",
    "\n",
    "                # Predict class-1 probabilities for the current bin\n",
    "                proba = model.predict_proba(X)[:, 1]\n",
    "                proba_by_bin[key] = proba\n",
    "\n",
    "                # Threshold with a refractory (skip window) to prevent dense re-triggers\n",
    "                result = np.zeros_like(proba, dtype=int)\n",
    "                i = 0\n",
    "                while i < len(proba):\n",
    "                    if proba[i] >= threshold:\n",
    "                        result[i] = 1\n",
    "                        i += refractory\n",
    "                    else:\n",
    "                        i += 1\n",
    "                tr_by_bin[key] = result\n",
    "\n",
    "                # EDs per minute = detections per bin divided by bin duration in minutes\n",
    "                spm = result.sum() / (len(result) / samples_per_min)\n",
    "\n",
    "                rows.append({\n",
    "                    \"subject\": f\"{subj}_{part}\",\n",
    "                    \"EDs_per_min\": spm,\n",
    "                    \"group\": group_label\n",
    "                })\n",
    "\n",
    "                part += 1\n",
    "\n",
    "    # Process both groups using shared logic\n",
    "    process_group(\"HC\", subjects_HC, hc)\n",
    "    process_group(\"EPI\", subjects_EPI, epi)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"subject\", \"EDs_per_min\", \"group\"])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6016482d337268c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.7    # probability threshold for ED detection             \n",
    "model_im = joblib.load('zeeg_model.pkl') # loaded model\n",
    "bin_length = 20 # separate into 20 min bins\n",
    "sec_limit = 2   # refractory period in seconds\n",
    "\n",
    "zeeg1_id = 'E227'\n",
    "zeeg2_id = 'E254'\n",
    "df = extract_rate_per_bin(model_im, zeeg1_id, zeeg2_id, threshold, bin_length, sec_limit)\n",
    "# save as csv\n",
    "df.to_csv('zeeg_eds_per_min.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aa4ff00dbf43983"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
