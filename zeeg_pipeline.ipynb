{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-21T19:27:43.674343Z",
     "start_time": "2025-09-21T19:27:43.652386Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "import joblib\n",
    "import mne\n",
    "import antropy as ant\n",
    "import scipy.stats as sp_stats\n",
    "import mne_features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6273004d5760491e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# General vars\n",
    "sr = 1000  # Hz\n",
    "window_size = 250 # samples\n",
    "mtl_path = 'P%s_mtl_clean.fif'\n",
    "all_subjects = ['01', '02', '03'] # example ids\n",
    "depth_model = joblib.load(r'depth_model.pkl')\n",
    "depth_channels = ['RAH1', 'LAH1', 'RA1', 'LA1', 'LEC1', 'REC1', 'RPHG1', 'LPHG1', 'RMH1', 'LMH1', 'RAH2', 'LAH2', 'RA2', 'LA2', 'LEC2', 'REC2', 'RPHG2', 'LPHG2', 'RMH2', 'LMH2']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34415b17fbe1bb7d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_depth_features(epochs, subj):\n",
    "    \"\"\"\n",
    "    Extracts features for the depth model from epoched data.\n",
    "\n",
    "    Args:\n",
    "        epochs (np.ndarray or List): 2D array-like structure (n_epochs, n_samples)\n",
    "                                     containing the signal windows.\n",
    "        subj (str): The subject identifier.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one row per epoch and one column per feature.\n",
    "    \"\"\"\n",
    "    mobility, complexity = ant.hjorth_params(epochs, axis=1)\n",
    "    feat = {\n",
    "        'subj': np.full(len(epochs), subj),\n",
    "        'epoch_id': np.arange(len(epochs)),\n",
    "        'kurtosis': sp_stats.kurtosis(epochs, axis=1),\n",
    "        'hjorth_mobility': mobility,\n",
    "        'hjorth_complexity': complexity,\n",
    "        'ptp_amp': np.ptp(epochs, axis=1),\n",
    "        'samp_entropy': np.apply_along_axis(ant.sample_entropy, axis=1, arr=epochs)\n",
    "    }\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    feat = pd.DataFrame(feat)\n",
    "    \n",
    "    # Teager-Kaiser energy\n",
    "    kaiser = mne_features.univariate.compute_teager_kaiser_energy(np.array(epochs))\n",
    "    reshaped_list = np.array(kaiser).reshape(-1, 12)\n",
    "    X_kaiser = pd.DataFrame(reshaped_list)\n",
    "    # rename columns\n",
    "    X_kaiser.columns = [\n",
    "        f'teager_kaiser_energy_{i}_mean' if j % 2 == 0 else f'teager_kaiser_energy_{i}_std'\n",
    "        for i in range(6) for j in range(2)\n",
    "    ]\n",
    "\n",
    "    feat = pd.concat([feat, X_kaiser], axis=1)\n",
    "    return feat\n",
    "\n",
    "def raw_chan_to_feat(raw, chan, subj, depth):\n",
    "    \"\"\"\n",
    "    Processes a single channel from a raw file, epochs it, and extracts features.\n",
    "\n",
    "    This function normalizes the channel, segments it into 250ms windows,\n",
    "    and calls the appropriate feature extraction function (zEEG or depth).\n",
    "    Finally, it adds global channel-level features to all epochs.\n",
    "\n",
    "    Args:\n",
    "        raw (mne.io.Raw): The MNE raw object.\n",
    "        chan (str): The name of the channel to process.\n",
    "        subj (str): The subject identifier.\n",
    "        depth (bool): Flag. If True, extract depth features.\n",
    "                      If False, extract zEEG features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of features, one row per valid epoch.\n",
    "    \"\"\"\n",
    "    epochs = []\n",
    "    chan_raw = raw.copy().pick([chan]).get_data(reject_by_annotation='NaN').flatten()\n",
    "    \n",
    "    # normalize chan\n",
    "    chan_norm = (chan_raw - np.nanmean(chan_raw)) / np.nanstd(chan_raw)\n",
    "    \n",
    "    # run over all 250ms epochs (exclude last second)\n",
    "    for i in range(0, len(chan_norm) - sr, window_size):\n",
    "        if not np.isnan(chan_norm[i: i + window_size]).any():\n",
    "            epochs.append(chan_norm[i: i + window_size])\n",
    "\n",
    "    if depth:\n",
    "        curr_feat = extract_depth_features(epochs, subj)\n",
    "    else: # zeeg\n",
    "        curr_feat = extract_zeeg_features(epochs, subj, raw.info['sfreq'])\n",
    "    \n",
    "    # add channel features for all epochs\n",
    "    chan_feat = {\n",
    "        'chan_name': chan,\n",
    "        'chan_ptp': np.ptp(chan_norm[~np.isnan(chan_norm)]),\n",
    "        'chan_skew': sp_stats.skew(chan_norm[~np.isnan(chan_norm)]),\n",
    "        'chan_kurt': sp_stats.kurtosis(chan_norm[~np.isnan(chan_norm)]),\n",
    "    }\n",
    "\n",
    "    for feat in chan_feat.keys():\n",
    "        curr_feat[feat] = chan_feat[feat]\n",
    "\n",
    "    return curr_feat\n",
    "\n",
    "def get_depth_pred(subjects, threshold=0.8, min_channels=2):\n",
    "    \"\"\"\n",
    "    Generates depth model predictions (labels) for a list of subjects.\n",
    "\n",
    "    This function iterates through subjects, loads data, extracts features\n",
    "    from target channels, and generates a binary prediction for each epoch\n",
    "    based on a consensus of channels.\n",
    "\n",
    "    Args:\n",
    "        subjects: List of subject IDs.\n",
    "        threshold (float): Probability threshold for a single channel to be considered \"active\".\n",
    "        min_channels (int): The minimum number of active channels required to\n",
    "                            label an epoch as positive (1).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are subject IDs and values are np.ndarrays\n",
    "              of binary predictions (one per epoch).\n",
    "    \"\"\"\n",
    "    y_all = {}\n",
    "    for subj in subjects:\n",
    "        raw = mne.io.read_raw(mtl_path % subj)\n",
    "        # Find channels that exist in *both* the raw file and our target list\n",
    "        curr_chans = [chan for chan in raw.ch_names if chan in depth_channels]\n",
    "        y_curr = None\n",
    "        # Predict and sum over channels\n",
    "        for chan in curr_chans:\n",
    "            curr_feat = raw_chan_to_feat(raw, chan, subj, depth=True)\n",
    "            predictions = depth_model.predict_proba(curr_feat[depth_model.get_booster().feature_names])\n",
    "            if y_curr is None:\n",
    "                y_curr = (predictions[:, 1] >= threshold).astype(int)\n",
    "            else:\n",
    "                y_curr += (predictions[:, 1] >= threshold).astype(int)\n",
    "\n",
    "        # at least X channels should be above threshold\n",
    "        y_curr[y_curr <= min_channels - 1] = 0\n",
    "        y_curr[y_curr > min_channels - 1] = 1\n",
    "        y_all[subj] = y_curr\n",
    "\n",
    "    return y_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84e0111c4579ece0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_zeeg_features(epochs, subj, sr):\n",
    "    \"\"\"\n",
    "    Extracts a comprehensive set of zEEG features from epoched data.\n",
    "\n",
    "    Calculates features across several domains:\n",
    "    - Basic statistical (mean, std, ptp, etc.)\n",
    "    - Fractal / nonlinear (Higuchi, Katz, Hjorth)\n",
    "    - Entropy (Sample, Spectral, SVD)\n",
    "    - Power (Absolute, Normalized band power, band ratios)\n",
    "    - Energy (Band energy, band ratios)\n",
    "    - Wavelet energy\n",
    "    - Teager-Kaiser energy\n",
    "\n",
    "    Args:\n",
    "        epochs (np.ndarray or List): 2D array-like (n_epochs, n_samples).\n",
    "        subj (str): The subject identifier.\n",
    "        sr (int): The sampling rate in Hz.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one row per epoch and one column per feature.\n",
    "    \"\"\"\n",
    "    data = np.array(epochs)\n",
    "    uni = mne_features.univariate\n",
    "\n",
    "    # Frequency bands configuration\n",
    "    bands = {\n",
    "        'theta': (4, 8), 'alpha': (8, 12), 'sigma': (12, 16),\n",
    "        'beta': (16, 30), 'gamma': (30, 100), 'fast': (100, 300)\n",
    "    }\n",
    "    band_list = ['theta', 'alpha', 'sigma', 'beta', 'gamma', 'fast']\n",
    "    psd_params = {'psd_method': 'welch', 'psd_params': None}\n",
    "\n",
    "    # === Basic statistical & complexity features ===\n",
    "    basic_features = {\n",
    "        'ptp_amp': uni.compute_ptp_amp(data),\n",
    "        'mean': uni.compute_mean(data),\n",
    "        'std': uni.compute_std(data),\n",
    "        'variance': uni.compute_variance(data),\n",
    "        'skewness': uni.compute_skewness(data),\n",
    "        'kurtosis': uni.compute_kurtosis(data),\n",
    "        'quantile': uni.compute_quantile(data, q=0.75),\n",
    "        'rms': uni.compute_rms(data),\n",
    "        'line_length': uni.compute_line_length(data),\n",
    "        'zero_crossings': uni.compute_zero_crossings(data, threshold=np.finfo(float).eps),\n",
    "    }\n",
    "\n",
    "    # === Fractal & nonlinear features ===\n",
    "    complexity_features = {\n",
    "        'higuchi_fd': uni.compute_higuchi_fd(data, kmax=10),\n",
    "        'katz_fd': uni.compute_katz_fd(data),\n",
    "        'hurst_exp': uni.compute_hurst_exp(data),\n",
    "        'hjorth_mobility': uni.compute_hjorth_mobility(data),\n",
    "        'hjorth_complexity': uni.compute_hjorth_complexity(data),\n",
    "        'hjorth_mobility_spect': uni.compute_hjorth_mobility_spect(sr, data, normalize=False, **psd_params),\n",
    "        'hjorth_complexity_spect': uni.compute_hjorth_complexity_spect(sr, data, normalize=False, **psd_params),\n",
    "    }\n",
    "\n",
    "    # === Entropy features ===\n",
    "    entropy_features = {\n",
    "        'app_entropy': uni.compute_app_entropy(data, emb=2, metric='chebyshev'),\n",
    "        'samp_entropy': uni.compute_samp_entropy(data, emb=2, metric='chebyshev'),\n",
    "        'spect_entropy': uni.compute_spect_entropy(sr, data, **psd_params),\n",
    "        'svd_entropy': uni.compute_svd_entropy(data, tau=2, emb=10),\n",
    "        'svd_fisher_info': uni.compute_svd_fisher_info(data, tau=2, emb=10),\n",
    "        'decorr_time': uni.compute_decorr_time(sr, data),\n",
    "    }\n",
    "\n",
    "    # === Power features ===\n",
    "    abspow = uni.compute_pow_freq_bands(sr, data, {'total': (0.1, 500)}, False, psd_method='multitaper')\n",
    "\n",
    "    # Combine basic features\n",
    "    df_basic = pd.DataFrame({**basic_features, **complexity_features, **entropy_features, 'abspow_': abspow})\n",
    "\n",
    "    # === Spectral slope ===\n",
    "    slope = uni.compute_spect_slope(sr, data, fmin=0.1, fmax=50, with_intercept=True, psd_method='welch')\n",
    "    df_slope = pd.DataFrame(\n",
    "        np.array(slope).reshape(-1, 4),\n",
    "        columns=['spect_slope_intercept', 'spect_slope_slope', 'spect_slope_MSE', 'spect_slope_R2']\n",
    "    )\n",
    "\n",
    "    # === Frequency band power (normalized) ===\n",
    "    pow_bands = uni.compute_pow_freq_bands(\n",
    "        data=data, sfreq=sr, freq_bands=bands, normalize=True,\n",
    "        ratios=None, psd_method='multitaper', log=False\n",
    "    )\n",
    "    df_pow = pd.DataFrame(\n",
    "        np.array(pow_bands).reshape(-1, len(bands)),\n",
    "        columns=[f'pow_freq_bands_{b}' for b in band_list]\n",
    "    )\n",
    "    # Add all band ratios\n",
    "    for b1 in band_list:\n",
    "        for b2 in band_list:\n",
    "            if b1 != b2:\n",
    "                df_pow[f'pow_freq_bands_{b1}/{b2}'] = df_pow[f'pow_freq_bands_{b1}'] / df_pow[f'pow_freq_bands_{b2}']\n",
    "\n",
    "    # === Frequency band energy ===\n",
    "    energy = uni.compute_energy_freq_bands(sr, data, freq_bands=bands)\n",
    "    df_energy = pd.DataFrame(\n",
    "        np.array(energy).reshape(-1, len(bands)),\n",
    "        columns=[f'energy_freq_bands_{b}' for b in band_list]\n",
    "    )\n",
    "\n",
    "    # Add two-letter abbreviation ratios\n",
    "    for b1 in band_list:\n",
    "        for b2 in band_list:\n",
    "            if b1 != b2 and f'energy_freq_bands_{b2[0]}{b1[0]}' not in df_energy.columns:\n",
    "                df_energy[f'energy_freq_bands_{b1[0]}{b2[0]}'] = (\n",
    "                    df_energy[f'energy_freq_bands_{b1}'] / df_energy[f'energy_freq_bands_{b2}']\n",
    "                )\n",
    "\n",
    "    # === Wavelet features ===\n",
    "    wave = uni.compute_wavelet_coef_energy(data, wavelet_name='db4')\n",
    "    df_wave = pd.DataFrame(\n",
    "        np.array(wave).reshape(-1, 5),\n",
    "        columns=[f'wavelet_coef_energy_{i}' for i in range(5)]\n",
    "    )\n",
    "\n",
    "    # === Teager-Kaiser energy ===\n",
    "    kaiser = uni.compute_teager_kaiser_energy(data)\n",
    "    df_kaiser = pd.DataFrame(\n",
    "        np.array(kaiser).reshape(-1, 12),\n",
    "        columns=[f'teager_kaiser_energy_{i}_{stat}' for i in range(6) for stat in ['mean', 'std']]\n",
    "    )\n",
    "\n",
    "    # === Combine all features ===\n",
    "    df = pd.concat([df_basic, df_slope, df_energy, df_kaiser, df_pow, df_wave], axis=1)\n",
    "    df.insert(0, 'subj', subj)\n",
    "    df.insert(1, 'epoch_id', np.arange(len(df)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_all_features_per_chan(chan, subjects):\n",
    "    \"\"\"\n",
    "    Runs feature extraction for a *single channel* across all subjects.\n",
    "\n",
    "    Iterates through a list of subjects, loads their raw data, and calls\n",
    "    raw_chan_to_feat to extract zEEG features for the specified channel.\n",
    "\n",
    "    Args:\n",
    "        chan (str): The name of the channel to extract features for (e.g., \"zeeg1\").\n",
    "        subjects (List[str]): List of subject IDs to process.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are subject IDs and values are the\n",
    "              feature DataFrames returned by raw_chan_to_feat.\n",
    "    \"\"\"\n",
    "    all_features = {}\n",
    "    for subj in subjects:\n",
    "        raw = mne.io.read_raw(mtl_path % subj)\n",
    "        curr_feat = raw_chan_to_feat(raw, chan, subj, depth=False)\n",
    "        all_features[subj] = curr_feat\n",
    "\n",
    "    return all_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "415a71ee795f30d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# run and create the pkl for the next model\n",
    "y_all = get_depth_pred(all_subjects)\n",
    "zeeg1_all = get_all_features_per_chan('zeeg1', all_subjects)\n",
    "zeeg2_all = get_all_features_per_chan('zeeg2', all_subjects)\n",
    "subj_data = {}\n",
    "for subj in all_subjects:\n",
    "    subj_data[subj] = {'zeeg1': zeeg1_all[subj], 'zeeg2': zeeg2_all[subj], 'y': y_all[subj]}\n",
    "\n",
    "# save for later analysis\n",
    "joblib.dump(subj_data, 'zeeg_training_data.pkl')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d591d2ef847867b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### zEEG model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b79ceb5f7a84518e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subj_data = joblib.load('zeeg_training_data.pkl')\n",
    "# combine all subjects\n",
    "symmetric = True\n",
    "x = pd.DataFrame()\n",
    "y = np.array([])\n",
    "for subj in all_subjects:\n",
    "    zeeg1_subj = subj_data[subj]['zeeg1']\n",
    "    zeeg2_subj = subj_data[subj]['zeeg2']\n",
    "    y_subj = subj_data[subj]['y']\n",
    "    zeeg1_subj.reset_index(drop=True, inplace=True)\n",
    "    zeeg2_subj.reset_index(drop=True, inplace=True)\n",
    "    x_subj = pd.concat([zeeg1_subj, zeeg2_subj], axis=1, ignore_index=True)\n",
    "    x_subj.columns = [f'zeeg1_{col}' for col in zeeg1_subj.columns] + [f'zeeg2_{col}' for col in zeeg2_subj.columns]\n",
    "    x = pd.concat([x, x_subj], ignore_index=True)\n",
    "    if symmetric:\n",
    "        x_sym = pd.concat([zeeg2_subj, zeeg1_subj], axis=1, ignore_index=True)\n",
    "        x_sym.columns = x_subj.columns\n",
    "        x = pd.concat([x, x_sym], ignore_index=True)\n",
    "        y = np.concatenate((y, y_subj))\n",
    "    y = np.concatenate((y, y_subj))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ed02ce56d69bf2a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# balance data in each subject\n",
    "x['pred'] = y\n",
    "sampled_data_0 = pd.DataFrame()\n",
    "sampled_data_1 = pd.DataFrame()\n",
    "max_samples = 3000\n",
    "for subj in all_subjects:\n",
    "    n_EDs = x[(x['zeeg1_subj'] == subj) & (x['pred'] == 1)].shape[0]\n",
    "    sample_count = min(max_samples, n_EDs)\n",
    "    # Get negative samples for this subject\n",
    "    sampled_data_0 = pd.concat([sampled_data_0, x[(x['zeeg1_subj'] == subj) & (x['pred'] == 0)].sample(sample_count, replace=True, random_state=8)])\n",
    "    # Get positive samples for this subject\n",
    "    sampled_data_1 = pd.concat([sampled_data_1, x[(x['zeeg1_subj'] == subj) & (x['pred'] == 1)].sample(sample_count, replace=True, random_state=8)])\n",
    "\n",
    "# --- Final Assembly of Balanced Data ---\n",
    "sampled_data = pd.concat([sampled_data_1, sampled_data_0], ignore_index=True)\n",
    "x = sampled_data.drop(columns='pred')\n",
    "y = sampled_data['pred']\n",
    "sampled_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "334df2d76cb2c4ef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Positive class ratio: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": "      accuracy  precision  sensitivity  specificity        f1    ROCAUC  \\\n0     0.691008   0.693682     0.684105     0.697911  0.688860  0.765648   \n1     0.693594   0.697521     0.683653     0.703534  0.690517  0.769405   \n2     0.691862   0.697729     0.677026     0.706697  0.687221  0.765583   \n3     0.693092   0.697901     0.680942     0.705241  0.689317  0.767562   \n4     0.687494   0.692928     0.673411     0.701576  0.683030  0.761794   \nmean  0.691410   0.695952     0.679827     0.702992  0.687789  0.765999   \n\n         PRAUC  \n0     0.772923  \n1     0.775377  \n2     0.771551  \n3     0.776140  \n4     0.767891  \nmean  0.772776  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>sensitivity</th>\n      <th>specificity</th>\n      <th>f1</th>\n      <th>ROCAUC</th>\n      <th>PRAUC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.691008</td>\n      <td>0.693682</td>\n      <td>0.684105</td>\n      <td>0.697911</td>\n      <td>0.688860</td>\n      <td>0.765648</td>\n      <td>0.772923</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.693594</td>\n      <td>0.697521</td>\n      <td>0.683653</td>\n      <td>0.703534</td>\n      <td>0.690517</td>\n      <td>0.769405</td>\n      <td>0.775377</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.691862</td>\n      <td>0.697729</td>\n      <td>0.677026</td>\n      <td>0.706697</td>\n      <td>0.687221</td>\n      <td>0.765583</td>\n      <td>0.771551</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.693092</td>\n      <td>0.697901</td>\n      <td>0.680942</td>\n      <td>0.705241</td>\n      <td>0.689317</td>\n      <td>0.767562</td>\n      <td>0.776140</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.687494</td>\n      <td>0.692928</td>\n      <td>0.673411</td>\n      <td>0.701576</td>\n      <td>0.683030</td>\n      <td>0.761794</td>\n      <td>0.767891</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.691410</td>\n      <td>0.695952</td>\n      <td>0.679827</td>\n      <td>0.702992</td>\n      <td>0.687789</td>\n      <td>0.765999</td>\n      <td>0.772776</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose features\n",
    "meta_data = ['subj', 'epoch_id', 'chan_name', 'epoch']\n",
    "x_feat = x[x.columns[~x.columns.str.contains('|'.join(meta_data))]]\n",
    "\n",
    "# Initialize containers\n",
    "metrics = {'accuracy': [], 'precision': [], 'sensitivity': [], 'specificity': [], 'f1': [], 'ROCAUC': [], 'PRAUC': []}\n",
    "# Store all predictions for aggregate plots\n",
    "all_y_true = []\n",
    "all_y_prob = []\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=8)\n",
    "for train_index, test_index in kf.split(x_feat, y):\n",
    "    model = xgb.XGBClassifier() \n",
    "    x_train_fold, x_test_fold = x_feat.iloc[train_index], x_feat.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "    \n",
    "    model.fit(x_train_fold, y_train_fold)\n",
    "    y_prob = model.predict_proba(x_test_fold)[:, 1]  # probabilities for class 1\n",
    "    y_pred = (y_prob > 0.5).astype(int)  # thresholding at 0.5\n",
    "    y_true = y_test_fold\n",
    "    all_y_true.extend(y_true)\n",
    "    all_y_prob.extend(y_prob)\n",
    "\n",
    "    # save metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics['accuracy'].append(accuracy_score(y_true, y_pred))\n",
    "    metrics['precision'].append(precision_score(y_true, y_pred))\n",
    "    metrics['sensitivity'].append(recall_score(y_true, y_pred))\n",
    "    metrics['f1'].append(f1_score(y_true, y_pred))\n",
    "    metrics['specificity'].append(tn / (tn + fp))\n",
    "    metrics['ROCAUC'].append(roc_auc_score(y_true, y_prob))\n",
    "    metrics['PRAUC'].append(average_precision_score(y_true, y_prob))\n",
    "\n",
    "# Create results table\n",
    "results = pd.DataFrame(metrics)\n",
    "results.loc['mean'] = results.mean()\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-28T13:54:15.732764Z",
     "start_time": "2025-07-28T13:48:43.885351Z"
    }
   },
   "id": "a274da9098faa50a",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot ROC and PR curves\n",
    "fpr, tpr, _ = roc_curve(all_y_true, all_y_prob)\n",
    "precision, recall, _ = precision_recall_curve(all_y_true, all_y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd176fd1ffd137a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Application on non invasive data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11190aadb3a0dfe2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subjects_HC = ['HC1', 'HC2', 'HC3']  # example healthy control ids\n",
    "subjects_EPI = ['EPI1', 'EPI2', 'EPI3']  # example epilepsy patient ids\n",
    "# assume features are ready using the get_zeeg_features function\n",
    "hc = joblib.load(\"v1_HC.pkl\")\n",
    "epi = joblib.load(\"v1_EPI.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20fb052deef8ba5f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_rate_per_bin(model, channel_1, channel_2, threshold, bin_length, sec_limit):\n",
    "    \"\"\"\n",
    "    Computes EDs-per-minute (SPM) for HC and EPI groups from feature data.\n",
    "\n",
    "    This function iterates through subjects, bins their data, runs the model,\n",
    "    applies a refractory period, and calculates the SPM for each bin.\n",
    "\n",
    "    Args:\n",
    "        model: The trained XGBoost model object.\n",
    "        channel_1 (str): Name of the first zeeg channel key (e.g., 'E227').\n",
    "        channel_2 (str): Name of the second zeeg channel key (e.g., 'E254').\n",
    "        threshold (float): Probability threshold for ED detection (0 to 1).\n",
    "        bin_length_min (int): Duration of each analysis bin in minutes.\n",
    "        refractory_sec (int): Refractory period in seconds after a detection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with [\"subject\", \"EDs_per_min\", \"group\"]\n",
    "                      for each bin of each subject.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling assumptions: 4 Hz -> 240 samples per minute\n",
    "    sampling_hz = 4\n",
    "    samples_per_min = sampling_hz * 60  # 240\n",
    "    bin_stride = bin_length * samples_per_min\n",
    "    refractory = int(sec_limit * sampling_hz)  # skip window after a detection\n",
    "\n",
    "    # Optional caches for downstream analysis, if needed later\n",
    "    proba_by_bin = {}  # (group, subj, part) -> np.ndarray of P(class=1)\n",
    "    tr_by_bin = {}     # (group, subj, part) -> np.ndarray of 0/1 after threshold + refractory\n",
    "\n",
    "    rows = []  # rows for the output DataFrame\n",
    "\n",
    "    def process_group(group_label, subjects, store):\n",
    "        \"\"\"\n",
    "        Run inference per subject in a group, split into bins, threshold with refractory,\n",
    "        and accumulate EDs-per-minute for each bin into rows.\n",
    "        \"\"\"\n",
    "        for subj in subjects:\n",
    "            # Build feature matrix for subject by concatenating zEEG channels\n",
    "            zeeg1 = store[subj][channel_1]\n",
    "            zeeg2 = store[subj][channel_2]\n",
    "            curr_feat = pd.concat([zeeg1, zeeg2], axis=1, ignore_index=True)\n",
    "            curr_feat.columns = [f\"zeeg1_{c}\" for c in zeeg1.columns] + [f\"zeeg2_{c}\" for c in zeeg2.columns]\n",
    "\n",
    "            # Enforce exact feature order known to the trained model\n",
    "            feat_names = model.get_booster().feature_names\n",
    "            X_all = curr_feat[feat_names]\n",
    "\n",
    "            part = 0\n",
    "            n = len(X_all)\n",
    "            for start in range(0, n, bin_stride):\n",
    "                stop = start + bin_stride\n",
    "                X = X_all.iloc[start:stop, :]\n",
    "\n",
    "                # Skip very short tail bins (< 50% of target size) to reduce noise\n",
    "                if len(X) < (bin_stride // 2):\n",
    "                    continue\n",
    "\n",
    "                key = (group_label, subj, part)\n",
    "\n",
    "                # Predict class-1 probabilities for the current bin\n",
    "                proba = model.predict_proba(X)[:, 1]\n",
    "                proba_by_bin[key] = proba\n",
    "\n",
    "                # Threshold with a refractory (skip window) to prevent dense re-triggers\n",
    "                result = np.zeros_like(proba, dtype=int)\n",
    "                i = 0\n",
    "                while i < len(proba):\n",
    "                    if proba[i] >= threshold:\n",
    "                        result[i] = 1\n",
    "                        i += refractory\n",
    "                    else:\n",
    "                        i += 1\n",
    "                tr_by_bin[key] = result\n",
    "\n",
    "                # EDs per minute = detections per bin divided by bin duration in minutes\n",
    "                spm = result.sum() / (len(result) / samples_per_min)\n",
    "\n",
    "                rows.append({\n",
    "                    \"subject\": f\"{subj}_{part}\",\n",
    "                    \"EDs_per_min\": spm,\n",
    "                    \"group\": group_label\n",
    "                })\n",
    "\n",
    "                part += 1\n",
    "\n",
    "    # Process both groups using shared logic\n",
    "    process_group(\"HC\", subjects_HC, hc)\n",
    "    process_group(\"EPI\", subjects_EPI, epi)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"subject\", \"EDs_per_min\", \"group\"])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6016482d337268c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "threshold = 0.7    # probability threshold for ED detection             \n",
    "model_im = joblib.load('zeeg_model.pkl') # loaded model\n",
    "bin_length = 20 # separate into 20 min bins\n",
    "sec_limit = 2   # refractory period in seconds\n",
    "\n",
    "zeeg1_id = 'E227'\n",
    "zeeg2_id = 'E254'\n",
    "df = extract_rate_per_bin(model_im, zeeg1_id, zeeg2_id, threshold, bin_length, sec_limit)\n",
    "# save as csv\n",
    "df.to_csv('zeeg_eds_per_min.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aa4ff00dbf43983"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
